# Radar - Implementation Guide for Vibe Coder

Este documento detalha as 5 implementa√ß√µes priorit√°rias para melhorar o core do Radar. Cada se√ß√£o √© auto-contida com contexto, especifica√ß√£o t√©cnica e crit√©rios de aceite.

---

## Sum√°rio de Prioridades

| # | Item | Esfor√ßo | Impacto | Depend√™ncias |
|---|------|---------|---------|--------------|
| 1 | Product Configuration UI | M√©dio | Alto | Nenhuma |
| 2 | Embedding Context Expandido | Baixo | Alto | #1 |
| 3 | Truncation Inteligente | M√©dio | M√©dio | Nenhuma |
| 4 | Prompt de AI Estruturado | Baixo | Alto | Nenhuma |
| 5 | Minimum Fit para AI Trigger | Baixo | M√©dio | Nenhuma |

---

# 1. Product Configuration UI

## Contexto

Atualmente os produtos s√£o configurados via c√≥digo em `products.py`. Para escalar e permitir self-service, precisamos de uma UI para CRUD de produtos.

## O que construir

### 1.1 Nova p√°gina/se√ß√£o: `/settings/products`

Acess√≠vel via sidebar ou menu de configura√ß√µes.

### 1.2 Lista de Produtos

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ‚öôÔ∏è Products                                    [+ Add New] ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ üì¶ PROFITDOCTOR                          [Edit] [Delete]‚îÇ‚îÇ
‚îÇ  ‚îÇ Profit tracking for Shopify                             ‚îÇ‚îÇ
‚îÇ  ‚îÇ 5 pain signals ¬∑ 4 intents ¬∑ 3 subreddits              ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ üì¶ SOCIALGENIUS                          [Edit] [Delete]‚îÇ‚îÇ
‚îÇ  ‚îÇ AI content creation for social media                    ‚îÇ‚îÇ
‚îÇ  ‚îÇ 4 pain signals ¬∑ 3 intents ¬∑ 4 subreddits              ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 1.3 Modal/P√°gina de Edi√ß√£o

Campos do formul√°rio:

| Campo | Tipo | Valida√ß√£o | Obrigat√≥rio |
|-------|------|-----------|-------------|
| `name` | text input | 2-50 chars, √∫nico | ‚úÖ |
| `description` | textarea | 50-500 chars | ‚úÖ |
| `pain_signals` | array de strings | m√≠n 3 items, cada 5-100 chars | ‚úÖ |
| `intent_signals` | array de strings | m√≠n 2 items, cada 5-100 chars | ‚úÖ |
| `target_subreddits` | multi-select/tags | m√≠n 1 | ‚úÖ |

### 1.4 Wireframe do Modal

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Edit Product                                          [X] ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                             ‚îÇ
‚îÇ  Name *                                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ ProfitDoctor                                           ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  Description * (be detailed - powers AI matching)           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ AI-powered profit diagnosis tool for Shopify           ‚îÇ‚îÇ
‚îÇ  ‚îÇ merchants. Tracks real profit margins, COGS,           ‚îÇ‚îÇ
‚îÇ  ‚îÇ shipping costs, and fees to show which products        ‚îÇ‚îÇ
‚îÇ  ‚îÇ actually make money.                                   ‚îÇ‚îÇ
‚îÇ  ‚îÇ                                                 120/500 ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îÇ  üí° Tip: Be specific about what problem you solve          ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  Pain Signals * (problems your product solves)              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ [don't know if profitable                         √ó]   ‚îÇ‚îÇ
‚îÇ  ‚îÇ [spreadsheet nightmare                            √ó]   ‚îÇ‚îÇ
‚îÇ  ‚îÇ [shipping costs eating margins                    √ó]   ‚îÇ‚îÇ
‚îÇ  ‚îÇ [hidden fees                                      √ó]   ‚îÇ‚îÇ
‚îÇ  ‚îÇ [which products make money                        √ó]   ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ Add new pain signal...                                 ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îÇ  [+ Add]                                                    ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  Intent Signals * (how users search for solutions)          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ [profit tracker shopify                           √ó]   ‚îÇ‚îÇ
‚îÇ  ‚îÇ [shopify analytics app                            √ó]   ‚îÇ‚îÇ
‚îÇ  ‚îÇ [margin calculator                                √ó]   ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ Add new intent signal...                               ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îÇ  [+ Add]                                                    ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  Target Subreddits *                                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ ‚òë r/shopify    ‚òë r/ecommerce    ‚òê r/dropship          ‚îÇ‚îÇ
‚îÇ  ‚îÇ ‚òê r/entrepreneur    ‚òê r/smallbusiness                 ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ Add custom subreddit: r/                               ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚ö° Embedding Preview (auto-generated)                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ ProfitDoctor: AI-powered profit diagnosis tool for     ‚îÇ‚îÇ
‚îÇ  ‚îÇ Shopify merchants...                                   ‚îÇ‚îÇ
‚îÇ  ‚îÇ                                                        ‚îÇ‚îÇ
‚îÇ  ‚îÇ Problems this product solves:                          ‚îÇ‚îÇ
‚îÇ  ‚îÇ - don't know if profitable                             ‚îÇ‚îÇ
‚îÇ  ‚îÇ - spreadsheet nightmare                                ‚îÇ‚îÇ
‚îÇ  ‚îÇ ...                                            [Expand]‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚ö†Ô∏è Saving will regenerate embeddings. This may take       ‚îÇ
‚îÇ     a few seconds.                                          ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  [Cancel]                    [Save & Regenerate Embedding]  ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Schema do Banco (SQLite)

```sql
-- Nova tabela (ou migrar de products.py)
CREATE TABLE IF NOT EXISTS products (
    id TEXT PRIMARY KEY,  -- slug: "profitdoctor"
    name TEXT NOT NULL,
    description TEXT NOT NULL,
    pain_signals TEXT NOT NULL,  -- JSON array: ["signal1", "signal2"]
    intent_signals TEXT NOT NULL,  -- JSON array
    target_subreddits TEXT NOT NULL,  -- JSON array
    embedding_context TEXT,  -- Texto gerado para embedding
    embedding_id TEXT,  -- Refer√™ncia no ChromaDB
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- √çndice para busca r√°pida
CREATE INDEX IF NOT EXISTS idx_products_name ON products(name);
```

## API Endpoints (FastAPI)

```python
# GET /api/products
# Lista todos os produtos
# Response: [{ id, name, description, pain_signals_count, intent_signals_count, subreddits_count }]

# GET /api/products/{id}
# Detalhes de um produto
# Response: { id, name, description, pain_signals, intent_signals, target_subreddits, embedding_context }

# POST /api/products
# Criar novo produto
# Body: { name, description, pain_signals, intent_signals, target_subreddits }
# Response: { id, embedding_regenerated: true }

# PUT /api/products/{id}
# Atualizar produto
# Body: { name?, description?, pain_signals?, intent_signals?, target_subreddits? }
# Response: { id, embedding_regenerated: true/false }
# Nota: embedding_regenerated = true se description, pain_signals ou intent_signals mudaram

# DELETE /api/products/{id}
# Deletar produto
# Response: { deleted: true }
# Nota: Tamb√©m remove embedding do ChromaDB
```

## L√≥gica de Neg√≥cio

### Quando regenerar embedding:

```python
def should_regenerate_embedding(old_product, new_product) -> bool:
    """Retorna True se precisa regenerar embedding"""
    return (
        old_product.description != new_product.description or
        old_product.pain_signals != new_product.pain_signals or
        old_product.intent_signals != new_product.intent_signals
    )
    # N√ÉO regenera se s√≥ mudou: name, target_subreddits
```

### Gerar embedding context:

```python
def generate_embedding_context(product: Product) -> str:
    """Gera o texto que ser√° convertido em embedding"""
    pain_list = "\n- ".join(product.pain_signals)
    intent_list = "\n- ".join(product.intent_signals)
    
    return f"""{product.name}: {product.description}

Problems this product solves:
- {pain_list}

How users search for solutions:
- {intent_list}""".strip()
```

## Crit√©rios de Aceite

- [ ] P√°gina de listagem de produtos funcional
- [ ] Modal/p√°gina de cria√ß√£o de produto funcional
- [ ] Modal/p√°gina de edi√ß√£o de produto funcional
- [ ] Dele√ß√£o de produto com confirma√ß√£o
- [ ] Valida√ß√µes client-side e server-side
- [ ] Preview do embedding context em tempo real
- [ ] Embedding regenerado automaticamente ao salvar (quando necess√°rio)
- [ ] Dropdown de "Active Product" no Dashboard atualiza dinamicamente
- [ ] Migra√ß√£o dos produtos existentes de `products.py` para o banco

---

# 2. Embedding Context Expandido

## Contexto

Atualmente o embedding do produto √© gerado apenas com `"{name}: {description}"`. Isso limita a capacidade de matching sem√¢ntico.

## O que mudar

### Atual (limitado):

```python
product_embedding_text = f"{product.name}: {product.description}"
# Exemplo: "ProfitDoctor: Profit tracking for Shopify"
```

### Novo (expandido):

```python
def generate_embedding_context(product: Product) -> str:
    pain_list = "\n- ".join(product.pain_signals)
    intent_list = "\n- ".join(product.intent_signals)
    
    return f"""{product.name}: {product.description}

Problems this product solves:
- {pain_list}

How users search for solutions:
- {intent_list}""".strip()
```

### Exemplo de output:

```
ProfitDoctor: AI-powered profit diagnosis tool for Shopify merchants. 
Tracks real profit margins, COGS, shipping costs, and fees to show 
which products actually make money.

Problems this product solves:
- don't know if I'm actually profitable
- spreadsheet nightmare tracking costs
- shipping costs eating into margins
- hidden fees killing profit
- can't tell which products make money

How users search for solutions:
- profit tracker for shopify
- shopify analytics app
- margin calculator ecommerce
- profit dashboard
```

## Onde implementar

Arquivo: `radar/products.py` ou novo `radar/services/product_service.py`

```python
class ProductService:
    def __init__(self, db, chroma_client):
        self.db = db
        self.chroma = chroma_client
    
    def generate_embedding_context(self, product: Product) -> str:
        """Gera texto expandido para embedding"""
        pain_list = "\n- ".join(product.pain_signals)
        intent_list = "\n- ".join(product.intent_signals)
        
        return f"""{product.name}: {product.description}

Problems this product solves:
- {pain_list}

How users search for solutions:
- {intent_list}""".strip()
    
    def regenerate_embedding(self, product: Product) -> str:
        """Regenera embedding do produto no ChromaDB"""
        context = self.generate_embedding_context(product)
        
        # Gerar embedding via OpenAI
        embedding = self.openai.embeddings.create(
            model="text-embedding-3-small",
            input=context
        ).data[0].embedding
        
        # Atualizar no ChromaDB
        self.chroma.upsert(
            ids=[f"product_{product.id}"],
            embeddings=[embedding],
            metadatas=[{"type": "product", "product_id": product.id}]
        )
        
        # Salvar context no banco
        product.embedding_context = context
        self.db.save(product)
        
        return context
```

## Crit√©rios de Aceite

- [ ] Fun√ß√£o `generate_embedding_context` implementada
- [ ] Embedding usa o contexto expandido (n√£o s√≥ name:description)
- [ ] Context √© salvo no banco para debug/visualiza√ß√£o
- [ ] Preview do context aparece na UI de edi√ß√£o de produto
- [ ] Produtos existentes migrados para novo formato

---

# 3. Truncation Inteligente

## Contexto

O Unified Context (Title + Body + ALL Comments) pode exceder 8,191 tokens do `text-embedding-3-small`. Atualmente o modelo trunca silenciosamente o final - perdendo os comments mais recentes.

## O que implementar

### 3.1 Fun√ß√£o de contagem de tokens

```python
import tiktoken

def count_tokens(text: str, model: str = "text-embedding-3-small") -> int:
    """Conta tokens de um texto para o modelo especificado"""
    encoding = tiktoken.encoding_for_model(model)
    return len(encoding.encode(text))
```

### 3.2 Fun√ß√£o de truncation inteligente

```python
MAX_EMBEDDING_TOKENS = 7500  # Buffer de seguran√ßa (modelo suporta 8191)
RESERVED_FOR_TITLE_BODY = 1500  # Reserva para title + body

def build_unified_context(post: Post, comments: list[Comment]) -> str:
    """
    Constr√≥i unified context com truncation inteligente.
    Prioriza: Title > Body > Top Comments por score
    """
    
    # 1. Sempre incluir title e body
    base_context = f"Title: {post.title}\n\nBody: {post.body or '(no body)'}"
    base_tokens = count_tokens(base_context)
    
    # 2. Calcular tokens dispon√≠veis para comments
    available_tokens = MAX_EMBEDDING_TOKENS - base_tokens
    
    if available_tokens <= 0:
        # Title + Body j√° excede limite (raro)
        # Truncar body mantendo in√≠cio
        return truncate_to_tokens(base_context, MAX_EMBEDDING_TOKENS)
    
    # 3. Ordenar comments por score (mais votados primeiro)
    sorted_comments = sorted(comments, key=lambda c: c.score, reverse=True)
    
    # 4. Adicionar comments at√© atingir limite
    included_comments = []
    current_tokens = base_tokens
    
    for comment in sorted_comments:
        comment_text = f"\n\nComment (score {comment.score}): {comment.body}"
        comment_tokens = count_tokens(comment_text)
        
        if current_tokens + comment_tokens > MAX_EMBEDDING_TOKENS:
            # N√£o cabe mais
            break
        
        included_comments.append(comment_text)
        current_tokens += comment_tokens
    
    # 5. Montar contexto final
    unified_context = base_context + "".join(included_comments)
    
    # 6. Log se houve truncation
    if len(included_comments) < len(sorted_comments):
        logger.info(
            f"Truncation applied: {len(included_comments)}/{len(sorted_comments)} "
            f"comments included ({current_tokens} tokens)"
        )
    
    return unified_context


def truncate_to_tokens(text: str, max_tokens: int) -> str:
    """Trunca texto para n√£o exceder max_tokens"""
    encoding = tiktoken.encoding_for_model("text-embedding-3-small")
    tokens = encoding.encode(text)
    
    if len(tokens) <= max_tokens:
        return text
    
    truncated_tokens = tokens[:max_tokens]
    return encoding.decode(truncated_tokens)
```

### 3.3 Onde aplicar

No pipeline de processamento, antes de gerar embedding:

```python
# Em radar/process/embeddings.py ou similar

def process_post(post: Post) -> None:
    # Buscar comments do banco
    comments = db.get_comments_for_post(post.id)
    
    # ANTES (problem√°tico):
    # unified_context = f"{post.title}\n{post.body}\n" + "\n".join([c.body for c in comments])
    
    # DEPOIS (com truncation inteligente):
    unified_context = build_unified_context(post, comments)
    
    # Gerar embedding
    embedding = openai.embeddings.create(
        model="text-embedding-3-small",
        input=unified_context
    ).data[0].embedding
    
    # Salvar...
```

### 3.4 Adicionar coluna de metadata (opcional mas recomendado)

```sql
ALTER TABLE posts ADD COLUMN context_tokens INTEGER;
ALTER TABLE posts ADD COLUMN comments_included INTEGER;
ALTER TABLE posts ADD COLUMN truncation_applied BOOLEAN DEFAULT FALSE;
```

## Crit√©rios de Aceite

- [ ] Fun√ß√£o `count_tokens` implementada usando tiktoken
- [ ] Fun√ß√£o `build_unified_context` implementada
- [ ] Comments s√£o ordenados por score antes de incluir
- [ ] Truncation mant√©m title + body completos (se poss√≠vel)
- [ ] Log quando truncation √© aplicada
- [ ] Metadata de truncation salva no banco (opcional)
- [ ] Testado com post real de 500+ comments

---

# 4. Prompt de AI Estruturado

## Contexto

O prompt atual √© gen√©rico e retorna Markdown n√£o estruturado. Precisamos de output JSON consistente com campos √∫teis para o produto.

## Prompt Atual (problem√°tico)

```python
system_prompt = "You are a product analyst helping a founder find leads and insights."

user_prompt = """
1. Assessment of pain point.
2. Product solution.
3. Fit rating (1-10).
"""
```

### Problemas:
- N√£o passa contexto do produto
- Output n√£o √© parse√°vel
- "Fit rating 1-10" duplica o Fit score que j√° temos
- N√£o sugere √¢ngulo de resposta

## Novo Prompt (estruturado)

```python
SYSTEM_PROMPT = """You are a lead qualification analyst for SaaS products.
Your job is to analyze Reddit discussions and identify sales opportunities.

Rules:
1. Be specific - quote exact phrases from the discussion
2. Focus on actionable insights
3. Always respond in valid JSON format
4. Never invent information not present in the discussion"""


def build_analysis_prompt(post: Post, product: Product) -> str:
    """Constr√≥i prompt para an√°lise de AI"""
    
    # Pegar top 5 comments por relev√¢ncia
    top_comments = get_top_comments(post.id, limit=5)
    comments_text = "\n".join([
        f"- [{c.score} pts]: {c.body[:300]}" 
        for c in top_comments
    ])
    
    return f"""## Product Context
Name: {product.name}
Description: {product.description}
Key problems it solves: {", ".join(product.pain_signals[:5])}

## Discussion to Analyze
Subreddit: r/{post.subreddit}
Title: {post.title}
Body: {post.body or "(no body)"}

Top Comments:
{comments_text}

## Your Task
Analyze this discussion for sales potential. Respond with this exact JSON structure:

{{
  "pain_point_summary": "One sentence describing the specific pain expressed",
  "pain_quote": "Exact quote from the post or comments that shows the pain (max 100 chars)",
  "urgency": "exploring | considering | actively_seeking | desperate",
  "product_relevance": "strong | moderate | weak | none",
  "relevance_explanation": "Why this product does or doesn't fit (1-2 sentences)",
  "response_angle": "Suggested hook/angle for responding to this person (1-2 sentences)",
  "confidence": 0.0-1.0
}}

Important:
- "pain_quote" must be an EXACT quote from the text, not paraphrased
- "urgency" should reflect how urgently they need a solution
- "response_angle" should be natural and helpful, not salesy
- "confidence" is your confidence in this analysis"""
```

## Parsing do Response

```python
import json
from typing import Optional
from pydantic import BaseModel, Field

class AIAnalysis(BaseModel):
    """Schema para an√°lise de AI"""
    pain_point_summary: str
    pain_quote: str
    urgency: str = Field(pattern="^(exploring|considering|actively_seeking|desperate)$")
    product_relevance: str = Field(pattern="^(strong|moderate|weak|none)$")
    relevance_explanation: str
    response_angle: str
    confidence: float = Field(ge=0.0, le=1.0)


def parse_ai_response(response_text: str) -> Optional[AIAnalysis]:
    """
    Parseia response do AI para objeto estruturado.
    Retorna None se parsing falhar.
    """
    try:
        # Limpar poss√≠veis markdown code blocks
        cleaned = response_text.strip()
        if cleaned.startswith("```json"):
            cleaned = cleaned[7:]
        if cleaned.startswith("```"):
            cleaned = cleaned[3:]
        if cleaned.endswith("```"):
            cleaned = cleaned[:-3]
        
        data = json.loads(cleaned.strip())
        return AIAnalysis(**data)
    
    except (json.JSONDecodeError, ValueError) as e:
        logger.error(f"Failed to parse AI response: {e}")
        logger.debug(f"Raw response: {response_text}")
        return None


def analyze_post_with_ai(post: Post, product: Product) -> Optional[AIAnalysis]:
    """Executa an√°lise de AI e retorna resultado estruturado"""
    
    prompt = build_analysis_prompt(post, product)
    
    response = openai.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": prompt}
        ],
        temperature=0.3,  # Baixa para output consistente
        max_tokens=500
    )
    
    result = parse_ai_response(response.choices[0].message.content)
    
    if result is None:
        # Retry uma vez com temperatura 0
        response = openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": prompt}
            ],
            temperature=0,
            max_tokens=500
        )
        result = parse_ai_response(response.choices[0].message.content)
    
    return result
```

## Atualizar Schema do Banco

```sql
-- Atualizar coluna de AI analysis para JSON estruturado
-- (se estava salvando como texto)

-- Op√ß√£o 1: Nova tabela
CREATE TABLE IF NOT EXISTS post_ai_analysis (
    post_id TEXT PRIMARY KEY,
    product_id TEXT NOT NULL,
    pain_point_summary TEXT,
    pain_quote TEXT,
    urgency TEXT,
    product_relevance TEXT,
    relevance_explanation TEXT,
    response_angle TEXT,
    confidence REAL,
    raw_response TEXT,  -- Backup do response original
    analyzed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (post_id) REFERENCES posts(id),
    FOREIGN KEY (product_id) REFERENCES products(id)
);

-- Op√ß√£o 2: Manter na tabela existente como JSON
ALTER TABLE post_analysis ADD COLUMN ai_analysis_json TEXT;
```

## Atualizar UI para mostrar novos campos

Na √°rea "AI INSIGHT FOR PROFITDOCTOR":

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ‚ö° AI Insight                                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                             ‚îÇ
‚îÇ üéØ Pain Point                                               ‚îÇ
‚îÇ User frustrated with spending hours on spreadsheets to     ‚îÇ
‚îÇ track profitability without clear answers.                 ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ üí¨ Key Quote                                                ‚îÇ
‚îÇ "I'm spending 10 hours/week and still don't know which    ‚îÇ
‚îÇ products to kill"                                          ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ üî• Urgency: ACTIVELY SEEKING                               ‚îÇ
‚îÇ üìä Relevance: STRONG (87% confidence)                      ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ üí° Response Angle                                           ‚îÇ
‚îÇ "Empathize with the spreadsheet pain, share how automated ‚îÇ
‚îÇ profit tracking changed the game. Offer to show specific   ‚îÇ
‚îÇ example."                                                   ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Crit√©rios de Aceite

- [ ] Novo prompt implementado com contexto do produto
- [ ] Output √© JSON estruturado
- [ ] Parsing com Pydantic e valida√ß√£o de schema
- [ ] Retry com temperature=0 se parsing falhar
- [ ] Campo `response_angle` inclu√≠do e √∫til
- [ ] Campo `pain_quote` extrai cita√ß√£o real
- [ ] UI atualizada para mostrar novos campos
- [ ] Banco atualizado para salvar JSON estruturado

---

# 5. Minimum Fit para AI Trigger

## Contexto

Atualmente o AI trigger √©:
```python
if total_relevance >= 7.0:
    trigger_ai_analysis()
```

O problema: um post pode ter Fit baixo (0.2) mas Intensity alta (10) e passar do threshold. Isso gasta tokens em leads de baixa qualidade.

### Exemplo problem√°tico:

| Post | Fit | Intensity | Intent | Relevance | AI Triggered? |
|------|-----|-----------|--------|-----------|---------------|
| Post viral irrelevante | 0.2 | 12 | 0 | 3+12+0 = 15 | ‚úÖ (errado!) |
| Post relevante novo | 0.7 | 2 | +5 | 10.5+2+5 = 17.5 | ‚úÖ (correto) |

## O que implementar

### L√≥gica atualizada

```python
# Configura√ß√µes (em config ou products.py)
AI_ANALYSIS_THRESHOLD = 7.0
AI_MINIMUM_FIT = 0.4  # Novo!

def should_trigger_ai_analysis(post_analysis: PostAnalysis) -> bool:
    """
    Determina se deve executar AI analysis.
    Requer AMBOS: relevance threshold E minimum fit.
    """
    return (
        post_analysis.total_relevance >= AI_ANALYSIS_THRESHOLD and
        post_analysis.semantic_similarity >= AI_MINIMUM_FIT
    )
```

### Onde aplicar

No pipeline de processamento:

```python
# Em radar/process/signals.py ou similar

def process_post_analysis(post: Post, product: Product) -> PostAnalysis:
    # ... calcular scores ...
    
    # Verificar se deve rodar AI
    # ANTES:
    # if analysis.total_relevance >= AI_ANALYSIS_THRESHOLD:
    #     analysis.ai_insight = analyze_with_ai(post, product)
    
    # DEPOIS:
    if should_trigger_ai_analysis(analysis):
        analysis.ai_insight = analyze_with_ai(post, product)
    else:
        analysis.ai_insight = None
        if analysis.total_relevance >= AI_ANALYSIS_THRESHOLD:
            # Log para debug: passou threshold mas n√£o fit
            logger.debug(
                f"Skipped AI for post {post.id}: "
                f"relevance={analysis.total_relevance:.1f} but fit={analysis.semantic_similarity:.2f}"
            )
    
    return analysis
```

### Configura√ß√£o por produto (opcional, avan√ßado)

```python
class Product:
    # ... outros campos ...
    ai_threshold: float = 7.0  # Default
    ai_minimum_fit: float = 0.4  # Default
```

Isso permite ajustar por produto se necess√°rio.

## Impacto Esperado

Baseado nos dados atuais, estimativa:

| M√©trica | Antes | Depois |
|---------|-------|--------|
| Posts analisados por AI | 100% dos > 7.0 | ~70% |
| Custo de AI por sync | $0.02 | ~$0.014 |
| Qualidade dos insights | Inclui irrelevantes | S√≥ relevantes |

## Crit√©rios de Aceite

- [ ] Constante `AI_MINIMUM_FIT` adicionada √† config
- [ ] Fun√ß√£o `should_trigger_ai_analysis` implementada
- [ ] L√≥gica de trigger atualizada em todos os lugares
- [ ] Log quando post √© skipado por fit baixo
- [ ] Dashboard mostra corretamente posts sem AI insight (n√£o √© erro)
- [ ] Testado com sync real e verificado economia de tokens

---

# Ordem de Implementa√ß√£o Recomendada

```
Semana 1:
‚îú‚îÄ‚îÄ #5 Minimum Fit para AI (30 min) ‚Üê Mais f√°cil, economiza $ imediatamente
‚îú‚îÄ‚îÄ #4 Prompt Estruturado (2-3h) ‚Üê Melhora qualidade dos insights
‚îî‚îÄ‚îÄ #3 Truncation (2-3h) ‚Üê Melhora qualidade dos embeddings

Semana 2:
‚îú‚îÄ‚îÄ #2 Embedding Expandido (1h) ‚Üê Depende de ter products no banco
‚îî‚îÄ‚îÄ #1 Product Config UI (4-6h) ‚Üê Maior esfor√ßo, mas essencial para self-service
```

## Checklist Final

### Antes de Deploy

- [ ] Todos os 5 items implementados
- [ ] Testes unit√°rios para fun√ß√µes cr√≠ticas
- [ ] Migra√ß√£o de produtos existentes
- [ ] Backup do banco antes de migra√ß√£o
- [ ] Testado com sync completo
- [ ] UI funcionando em todos os browsers

### M√©tricas para Validar Sucesso

| M√©trica | Baseline | Target |
|---------|----------|--------|
| Fit score m√©dio | ? | +10% |
| AI cost por sync | $0.02 | $0.015 |
| Posts com truncation | ? | <5% |
| AI insights com response_angle | 0% | 100% |

---

# Ap√™ndice: Arquivos que Ser√£o Modificados

```
radar/
‚îú‚îÄ‚îÄ config.py                 # Novas constantes
‚îú‚îÄ‚îÄ products.py               # Migrar para banco OU manter como fallback
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ product.py            # Pydantic model para Product
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ product_service.py    # NOVO: CRUD de produtos
‚îÇ   ‚îî‚îÄ‚îÄ embedding_service.py  # Modificar: context expandido
‚îú‚îÄ‚îÄ process/
‚îÇ   ‚îú‚îÄ‚îÄ embeddings.py         # Modificar: truncation
‚îÇ   ‚îî‚îÄ‚îÄ signals.py            # Modificar: AI trigger
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îî‚îÄ‚îÄ routes/
‚îÇ       ‚îî‚îÄ‚îÄ products.py       # NOVO: API endpoints
‚îî‚îÄ‚îÄ ui/
    ‚îî‚îÄ‚îÄ src/
        ‚îú‚îÄ‚îÄ pages/
        ‚îÇ   ‚îî‚îÄ‚îÄ ProductSettings.tsx  # NOVO: p√°gina de config
        ‚îî‚îÄ‚îÄ components/
            ‚îî‚îÄ‚îÄ ProductForm.tsx      # NOVO: formul√°rio
```

---

*Documento gerado em: Janeiro 2026*
*Vers√£o: 1.0*